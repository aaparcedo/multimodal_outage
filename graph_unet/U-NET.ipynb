{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 67, 3, 128, 128])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('grayscale_image.jpeg')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "image = transform(image)\n",
    "image = image.unsqueeze(0)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "image = image.expand(4, 67, 3, 128, 128)\n",
    "\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dimension = 128\n",
    "batch_size = 4\n",
    "n_counties = 67\n",
    "feature_vector_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False), \n",
    "            nn.ReLU(inplace=True),                                                     \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True)                                                       \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2))\n",
    "                \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contraction(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels): \n",
    "        super().__init__()\n",
    "        self.inc = (DoubleConv(in_channels, 4))\n",
    "        self.down1 = (Down(4, 8))\n",
    "        self.down2 = (Down(8, 16))\n",
    "        self.down3 = (Down(16, 32))\n",
    "        self.down4 = (Down(32, 64))\n",
    "        self.feature_maps = [[] for _ in range(4)]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        encoder_input = []\n",
    "        \n",
    "        for batch in range(batch_size):\n",
    "        \n",
    "            x1 = self.inc(input[batch])      \n",
    "            self.feature_maps[0].append(x1)            \n",
    "            \n",
    "            x2 = self.down1(x1)             \n",
    "            self.feature_maps[1].append(x2)\n",
    "            \n",
    "            x3 = self.down2(x2)            \n",
    "            self.feature_maps[2].append(x3)\n",
    "\n",
    "            x4 = self.down3(x3)            \n",
    "            self.feature_maps[3].append(x4)\n",
    "\n",
    "            x5 = self.down4(x4)             \n",
    "            encoder_input.append(x5)\n",
    "            \n",
    "        for feature_map in range(len(self.feature_maps)): \n",
    "            self.feature_maps[feature_map] = torch.stack(self.feature_maps[feature_map])\n",
    "              \n",
    "        encoder_input = torch.stack(encoder_input)\n",
    "        encoder_input = encoder_input.view(batch_size, n_counties, -1)\n",
    "        \n",
    "        return encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.downsized_image_dimension = image_dimension / 16\n",
    "        self.first_layer_size = int(self.downsized_image_dimension * self.downsized_image_dimension * 64)\n",
    "        self.fc1 = nn.Linear(self.first_layer_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.fc7 = nn.Linear(16, 8)\n",
    "\n",
    "    def forward(self, input):\n",
    "        wave_net_input = []\n",
    "        \n",
    "        for batch in range(batch_size):\n",
    "            x = torch.relu(self.fc1(input[batch]))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = torch.relu(self.fc3(x))\n",
    "            x = torch.relu(self.fc4(x))\n",
    "            x = torch.relu(self.fc5(x))\n",
    "            x = torch.relu(self.fc6(x))\n",
    "            x = self.fc7(x)\n",
    "            wave_net_input.append(x)\n",
    "            \n",
    "        wave_net_input = torch.stack(wave_net_input)\n",
    "        return wave_net_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.downsized_image_dimension = int(image_dimension / 16)\n",
    "        self.output_layer_size = int(self.downsized_image_dimension * self.downsized_image_dimension * 64)\n",
    "        self.fc1 = nn.Linear(feature_vector_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 256)\n",
    "        self.fc4 = nn.Linear(256, 512)\n",
    "        self.fc5 = nn.Linear(512, self.output_layer_size) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        expansion_input = []\n",
    "        \n",
    "        for batch in range(batch_size):\n",
    "            x = torch.relu(self.fc1(input[batch]))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = torch.relu(self.fc3(x))\n",
    "            x = torch.relu(self.fc4(x))\n",
    "            x = self.fc5(x)\n",
    "            expansion_input.append(x)\n",
    "            \n",
    "        expansion_input = torch.stack(expansion_input)\n",
    "        expansion_input = expansion_input.view(batch_size, n_counties, 64, self.downsized_image_dimension, self.downsized_image_dimension)\n",
    "        return expansion_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expansion(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_channels): \n",
    "        super(Expansion, self).__init__()\n",
    "        self.up1 = (Up(64, 32))\n",
    "        self.up2 = (Up(32, 16))\n",
    "        self.up3 = (Up(16, 8))\n",
    "        self.up4 = (Up(8, 4))\n",
    "        self.outc = (OutConv(4, output_channels))\n",
    "        \n",
    "    def forward(self, input, feature_maps):\n",
    "        predictions = []\n",
    "        feature_map_iteration = 0\n",
    "        \n",
    "        for batch in range(batch_size):\n",
    "            x = self.up1(input[batch], feature_maps[-1][feature_map_iteration])\n",
    "            x = self.up2(x, feature_maps[-2][feature_map_iteration])\n",
    "            x = self.up3(x, feature_maps[-3][feature_map_iteration])\n",
    "            x = self.up4(x, feature_maps[-4][feature_map_iteration])\n",
    "            logits = self.outc(x)\n",
    "            predictions.append(logits)\n",
    "            feature_map_iteration += 1\n",
    "            \n",
    "        predictions = torch.stack(predictions)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contraction: torch.Size([4, 67, 4096]) \n",
      "encoder: torch.Size([4, 67, 8])\n",
      "decoder: torch.Size([4, 67, 64, 8, 8])\n",
      "expansion: torch.Size([4, 67, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "contraction = Contraction(3)\n",
    "output = contraction(image)\n",
    "print(f\"contraction: {output.shape} \")\n",
    "\n",
    "encoder = Encoder()\n",
    "output = encoder(output)\n",
    "print(f\"encoder: {output.shape}\")\n",
    "\n",
    "decoder = Decoder()\n",
    "output = decoder(output)\n",
    "print(f\"decoder: {output.shape}\")\n",
    "\n",
    "expansion = Expansion(3)\n",
    "output = expansion(output, contraction.feature_maps)\n",
    "print(f\"expansion: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modified_UNET(nn.Module): \n",
    "    def __init__(self,input_channels=3, output_channels=3):\n",
    "        super(Modified_UNET, self).__init__()\n",
    "        self.contraction = Contraction(input_channels)\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.expansion = Expansion(output_channels)\n",
    "        \n",
    "    def forward(self, input): \n",
    "        output = self.contraction(input)\n",
    "        print(output.shape)\n",
    "        \n",
    "        output = self.encoder(output)\n",
    "        print(output.shape)\n",
    "        \n",
    "        output = self.decoder(output)\n",
    "        print(output.shape)\n",
    "        \n",
    "        feature_maps = self.contraction.feature_maps\n",
    "        predicted_results = self.expansion(output, feature_maps)\n",
    "        return predicted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 67, 4096])\n",
      "torch.Size([4, 67, 8])\n",
      "torch.Size([4, 67, 64, 8, 8])\n",
      "torch.Size([4, 67, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "model = Modified_UNET()\n",
    "output = model(image)\n",
    "print(output.shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
